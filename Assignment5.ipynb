{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 is on pdf"
      ],
      "metadata": {
        "id": "H_1yDc4-TmLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the MDP:\n",
        "\n",
        "**State Space (S):**\n",
        "\n",
        "Explanation:\n",
        "The state space consists of all possible grid cell configurations, i.e., each cell's current state (empty, blocked, or goal).\n",
        "\n",
        "S = {sij | 1 =< i, j <= 25}\n",
        "\n",
        "\n",
        "**Action Space (A):**\n",
        "\n",
        "Explanation:\n",
        "The action space consists of four possible actions - up, down, left, right - and a stay action.\n",
        "\n",
        "A = {'up', 'down', 'left', 'right','stay'}\n",
        "\n",
        "\n",
        "**Transition Probabilities (P):**\n",
        "\n",
        "Explanation:\n",
        "Given the deterministic nature of the grid world, the transition probabilities are straightforward. If an action leads to a valid state, the probability is 1; otherwise, it is 0.\n",
        "\n",
        " P(s'|s,a) = {1 if s' is a valid successor state\n",
        "              0 otherwise}\n",
        "\n",
        "\n",
        "**Reward Function (R):**\n",
        "\n",
        "Explanation:\n",
        "The agent incurs a reward of -1 for each time step until it reaches the goal state.\n",
        "\n",
        "R(s,a,s') = f(x) = { -1 if s' is the goal state\n",
        "                      0 otherwise}\n",
        "\n",
        "\n",
        "\n",
        "**Discount Factor (Î³):**\n",
        "\n",
        "Explanation:\n",
        "The discount factor represents the agent's preference for current rewards over future rewards. It is typically set between 0 and 1.\n",
        "\n",
        "0 <= gamma <= 1\n",
        "  \n",
        "\n",
        "gamma = 0.9"
      ],
      "metadata": {
        "id": "uFLXZgaO-aLu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "301akMGu-WX6",
        "outputId": "bc4cf59b-b243-476b-de64-96725c63a335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'down' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'right' 'up' 'left' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
            " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
            "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']]\n",
            "\n",
            "Optimal Value Function:\n",
            "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "   0.  0.  0.  0.  0.  0.  0.]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Initialize grid world\n",
        "\n",
        "grid_size = 25\n",
        "\n",
        "goal_state = (2, 5)\n",
        "#0,0\n",
        "#2,5\n",
        "#1,0\n",
        "#10,20\n",
        "#20,20\n",
        "#24,24 max\n",
        "\n",
        "blocked_states = [(i, j) for i in range(grid_size) for j in range(grid_size) if (i, j) != goal_state]\n",
        "\n",
        "# Define state space, action space, and discount factor\n",
        "\n",
        "states = [(i, j) for i in range(grid_size) for j in range(grid_size)]\n",
        "\n",
        "actions = ['up', 'down', 'left', 'right', 'stay']\n",
        "# Discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Initialize policy randomly\n",
        "\n",
        "policy = np.random.choice(actions, size=(grid_size, grid_size))\n",
        "\n",
        "# Define helper function to check if a state is valid\n",
        "\n",
        "def is_valid_state(state):\n",
        "\n",
        "    return state[0] >= 0 and state[0] < grid_size and state[1] >= 0 and state[1] < grid_size and state not in blocked_states\n",
        "\n",
        "# Implement policy evaluation\n",
        "\n",
        "def policy_evaluation(policy, gamma, threshold=1e-6):\n",
        "\n",
        "    # Initialize value function\n",
        "\n",
        "    V = np.zeros((grid_size, grid_size))\n",
        "\n",
        "\n",
        "\n",
        "    while True:\n",
        "\n",
        "        delta = 0\n",
        "\n",
        "        for i in range(grid_size):\n",
        "\n",
        "            for j in range(grid_size):\n",
        "\n",
        "                state = (i, j)\n",
        "\n",
        "                if state == goal_state:\n",
        "\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "                action = policy[i, j]\n",
        "\n",
        "                next_state = (i, j) if action == 'stay' else {'up': (i-1, j), 'down': (i+1, j), 'left': (i, j-1), 'right': (i, j+1)}[action]\n",
        "\n",
        "\n",
        "\n",
        "                if is_valid_state(next_state):\n",
        "\n",
        "                    reward = -1 if next_state == goal_state else 0\n",
        "\n",
        "                    new_value = reward + gamma * V[next_state[0], next_state[1]]\n",
        "\n",
        "                    delta = max(delta, np.abs(new_value - V[i, j]))\n",
        "\n",
        "                    V[i, j] = new_value\n",
        "\n",
        "\n",
        "\n",
        "        if delta < threshold:\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    return V\n",
        "\n",
        "# Implement policy improvement\n",
        "\n",
        "def policy_improvement(policy, V, gamma):\n",
        "\n",
        "    new_policy = np.copy(policy)\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(grid_size):\n",
        "\n",
        "        for j in range(grid_size):\n",
        "\n",
        "            state = (i, j)\n",
        "\n",
        "            if state == goal_state:\n",
        "\n",
        "                continue\n",
        "\n",
        "\n",
        "\n",
        "            action_values = []\n",
        "\n",
        "            for action in actions:\n",
        "\n",
        "                next_state = (i, j) if action == 'stay' else {'up': (i-1, j), 'down': (i+1, j), 'left': (i, j-1), 'right': (i, j+1)}[action]\n",
        "\n",
        "\n",
        "\n",
        "                if is_valid_state(next_state):\n",
        "\n",
        "                    reward = -1 if next_state == goal_state else 0\n",
        "\n",
        "                    action_values.append(reward + gamma * V[next_state[0], next_state[1]])\n",
        "\n",
        "                else:\n",
        "\n",
        "                    action_values.append(float('-inf'))\n",
        "\n",
        "\n",
        "\n",
        "            new_policy[i, j] = actions[np.argmax(action_values)]\n",
        "\n",
        "\n",
        "\n",
        "    return new_policy\n",
        "\n",
        "# Implement policy iteration\n",
        "\n",
        "def policy_iteration(gamma, threshold=1e-6):\n",
        "\n",
        "    policy = np.random.choice(actions, size=(grid_size, grid_size))\n",
        "\n",
        "\n",
        "\n",
        "    while True:\n",
        "\n",
        "        V = policy_evaluation(policy, gamma, threshold)\n",
        "\n",
        "        new_policy = policy_improvement(policy, V, gamma)\n",
        "\n",
        "\n",
        "\n",
        "        if np.array_equal(policy, new_policy):\n",
        "\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        policy = new_policy\n",
        "\n",
        "\n",
        "\n",
        "    return policy, V\n",
        "\n",
        "# Run policy iteration\n",
        "\n",
        "optimal_policy, optimal_value_function = policy_iteration(gamma)\n",
        "\n",
        "# Display results\n",
        "\n",
        "print(\"Optimal Policy:\")\n",
        "\n",
        "print(optimal_policy)\n",
        "\n",
        "print(\"\\nOptimal Value Function:\")\n",
        "\n",
        "print(optimal_value_function)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize the optimal policy\n",
        "\n",
        "plt.imshow(optimal_value_function, cmap='viridis', origin='lower')\n",
        "\n",
        "plt.title('Optimal Policy')\n",
        "\n",
        "plt.colorbar(label='Optimal Value Function')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a1g9Grb2AWBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a report describing your approach to the problem. It must contain the following:\n",
        "\n",
        "a. The choice of data structure to implement the MDP and a justification.\n",
        "\n",
        "**simple and straightforward and laid out easier to understand. Giant square made of 0's and ups wherever the spot is that is the goal the words around it change to make it more obvious**\n",
        "\n",
        "b. The performance of a random policy. Compute it by greedily evaluating it and reporting the median (out of three trials) number of steps it took to reach the goal state from 3 different start states. Randomly choose these states for your experiments.\n",
        "\n",
        "#2,5\n",
        "\n",
        "**Policy**\n",
        "\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'down' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 31\n",
        " ['up' 'up' 'up' 'up' 'right' 'up' 'left' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] right 55, left 57\n",
        "\n",
        "**optimal value**\n",
        "\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 50\n",
        " [ 0.  0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 75\n",
        " [ 0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 100\n",
        "56 average\n",
        "\n",
        "\n",
        "\n",
        "#1,0\n",
        "\n",
        "**policy**\n",
        "['down' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['down' 'left' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        "\n",
        "no stay\n",
        "\n",
        "**optimal**\n",
        "\n",
        "[-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.]1\n",
        " [ 0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 26\n",
        " [-1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 28\n",
        "\n",
        "26 average\n",
        "\n",
        "#10,20\n",
        "\n",
        "**policy**\n",
        "\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'down' 'up' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'right' 'stay' 'left' 'up' 'up' 'up'] 25\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        " ['up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up'\n",
        "  'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up' 'up']\n",
        "\n",
        "271 to stay\n",
        "\n",
        "**optimal**\n",
        "\n",
        "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0.  0.  0.  0.  0.  0.] 25\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0. -1.  0.  0.  0.  0.] 246\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0. -1.  0. -1.  0.  0.  0.] 270, 271 average 272\n",
        " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
        "   0.  0. -1.  0.  0.  0.  0.] 296\n",
        "\n",
        "271 average\n",
        "\n",
        "\n",
        "c. The performance of an intermediate policy obtained during policy iteration. Compute it by greedily evaluating it and reporting the median (out of three trials) number of steps it took to reach the goal state from 3 different start states. Use the same start states as defined in the previous section for your experiments.\n",
        "\n",
        "**There is a collection of up, right, left, stay, down that is displayed. 26 median**\n",
        "\n",
        "d. The performance of the optimal policy found using your approach. Compute it by greedily evaluating it and reporting the median (out of three trials) number of steps it took to reach the goal state from 3 different start states. Use the same start states as defined in the previous section for your experiments\n",
        "\n",
        "**optimal is 117.66 average**\n",
        "**median is 26**"
      ],
      "metadata": {
        "id": "Z07uz8cbBV74"
      }
    }
  ]
}
